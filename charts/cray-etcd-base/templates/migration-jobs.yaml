{{- /*
MIT License

(C) Copyright 2023 Hewlett Packard Enterprise Development LP

Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files (the "Software"),
to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense,
and/or sell copies of the Software, and to permit persons to whom the
Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.
*/}}

apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ include "cray-etcd-base.fullname" . }}-pre-upgrade-etcd-backup"
  labels:
    app.kubernetes.io/managed-by: {{ .Release.Service }}
  annotations:
    helm.sh/hook: pre-upgrade, pre-install
    helm.sh/hook-delete-policy: before-hook-creation
spec:
  ttlSecondsAfterFinished: 86400
  template:
    metadata:
      name: "{{ include "cray-etcd-base.fullname" . }}-pre-upgrade-etcd-backup"
      labels:
        app.kubernetes.io/managed-by: {{ .Release.Service }}
      annotations:
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      serviceAccountName: cray-etcd-migration
      volumes:
      - name: snapshot-volume
        persistentVolumeClaim:
          claimName: bitnami-etcd-snapshotter
      containers:
        - name: pre-upgrade-etcd-backup
          image: {{ .Values.util.image.repository }}:{{ .Values.util.image.tag }}
          imagePullPolicy: {{ .Values.util.image.pullPolicy }}
          volumeMounts:
          - name: snapshot-volume
            mountPath: /snapshots
          command:
            - '/bin/sh'
          args:
            - "-c"
            - |
               snapshot_dir="/snapshots/{{ include "cray-etcd-base.fullname" . }}-bitnami-etcd"
               snapshot_file="{{ include "cray-etcd-base.fullname" . }}_etcd_migration.snapshot-{{ .Release.Revision }}.db"
               echo "Creating $snapshot_dir directory for snapshots..."
               mkdir -p $snapshot_dir
               echo "Changing ownership of $snapshot_dir to etcd user"
               chown -R 1001:1001 $snapshot_dir
               ns={{ .Release.Namespace }}
               cluster="{{ include "cray-etcd-base.fullname" . }}"
               ss_name="${cluster}-bitnami-etcd"

               if ! kubectl get endpoints ${cluster}-etcd-client -n ${ns} -o json > /dev/null 2>&1 ; then
                 #
                 # There is no old etcd-operator managed chart installed, let's see if this
                 # is a fresh install or upgrade.
                 #
                 has_label=$(kubectl get statefulsets.apps -n services -l 'app.kubernetes.io/component=etcd' --no-headers 2>/dev/null | awk "/${ss_name}/")
                 if [ -n "$has_label" ]; then
                   #
                   # The new label has already been applied, we don't need to do
                   # anything special here.
                   #
                   echo "The 'app.kubernetes.io/component=etcd' label has already been applied to ${ss_name}, continuing..."
                   exit 0
                 fi

                 members=$(kubectl get pod -n $ns -o wide -o=custom-columns=NAME:.metadata.name | awk "/${ss_name}/ && !/snapshotter|defrag/")
                 if [ -n "$members"  ]; then
                   echo "Ensuring ${ss_name} members have 'app.kubernetes.io/component=etcd' label for bitnami 9.x chart upgrade..."
                   for member in ${members}; do
                     echo "Adding label to ${member}"
                     kubectl label pod -n ${ns} ${member} app.kubernetes.io/component=etcd --overwrite
                   done
                   echo "Deleting statefulset for ${ss_name} allowing K8S to recreate"
                   kubectl delete statefulset -n ${ns} ${ss_name} --cascade=orphan
                 else
                   echo "No previous installations detected requiring special handling..."
                 fi
                 exit 0
               fi

               echo "${cluster}-etcd-client found, proceeding with migrating current data..."

               cat > /tmp/upload-snapshot.py <<EOF
               #!/usr/bin/env python3

               import boto3
               import sys

               def main():
                   s3 = boto3.resource('s3', endpoint_url=sys.argv[1])
                   bucket = s3.Bucket('etcd-backup')
                   file_name="/tmp/%s" % (sys.argv[2])
                   key_name="{{ include "cray-etcd-base.fullname" . }}/%s" % (sys.argv[2])
                   bucket.upload_file(Filename=file_name, Key=key_name)

               if __name__ == '__main__':
                   main()
               EOF
               chmod 755 /tmp/upload-snapshot.py

               pods=$(kubectl get endpoints ${cluster}-etcd-client -n ${ns} -o jsonpath='{.subsets[*].addresses[*].targetRef.name}')
               for pod in $pods
               do
                 echo "Taking snapshot from ${pod}..."
                 kubectl exec -i -n ${ns} ${pod} -c etcd -- /bin/sh -c 'ETCDCTL_API=3 etcdctl snapshot save snapshot.db'
                 if [ $? -eq 0 ]; then
                   echo "Copying snapshot from ${pod} to /tmp/${snapshot_file}..."
                   kubectl -n ${ns} -c etcd cp $pod:snapshot.db /tmp/${snapshot_file}
                   if [ $? -eq 0 ]; then
                     break
                   fi
                 fi
               done

               echo "Grabbing keys and S3 endpoint from etcd-backup-s3-credentials..."
               access_key=$(kubectl get secret -n services etcd-backup-s3-credentials -ojsonpath='{.data.access_key}' | base64 -d)
               secret_key=$(kubectl get secret -n services etcd-backup-s3-credentials -ojsonpath='{.data.secret_key}' | base64 -d)
               http_s3_endpoint=$(kubectl get secret -n services  etcd-backup-s3-credentials -ojsonpath='{.data.http_s3_endpoint}' | base64 -d)

               mkdir -p /root/.aws
               cat > /root/.aws/credentials <<EOF
               [default]
               aws_access_key_id = $access_key
               aws_secret_access_key = $secret_key
               EOF

               cat > /root/.aws/config <<EOF
               [default]
               region=foo
               EOF

               echo "Uploading snapshot ${snapshot_file} to ${http_s3_endpoint}..."
               /tmp/upload-snapshot.py ${http_s3_endpoint} ${snapshot_file}
               echo "Done!"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ include "cray-etcd-base.fullname" . }}-etcd-post-install"
  labels:
    app.kubernetes.io/managed-by: {{ .Release.Service }}
  annotations:
    helm.sh/hook: post-upgrade, post-install
    helm.sh/hook-delete-policy: before-hook-creation
spec:
  ttlSecondsAfterFinished: 86400
  template:
    metadata:
      name: "{{ include "cray-etcd-base.fullname" . }}-etcd-post-install"
      labels:
        app.kubernetes.io/managed-by: {{ .Release.Service }}
      annotations:
        sidecar.istio.io/inject: "false"
        helm.sh/hook-weight: "-10"
    spec:
      restartPolicy: Never
      serviceAccountName: cray-etcd-migration
      volumes:
      - name: snapshot-volume
        persistentVolumeClaim:
          claimName: bitnami-etcd-snapshotter
      containers:
        - name: post-install
          image: {{ .Values.util.image.repository }}:{{ .Values.util.image.tag }}
          imagePullPolicy: {{ .Values.util.image.pullPolicy }}
          volumeMounts:
          - name: snapshot-volume
            mountPath: /snapshots
          command:
            - '/bin/sh'
          args:
            - '-c'
            - |
               ns={{ .Release.Namespace }}
               snapshot_dir="/snapshots/{{ include "cray-etcd-base.fullname" . }}-bitnami-etcd"
               snapshot_file="{{ include "cray-etcd-base.fullname" . }}_etcd_migration.snapshot-{{ .Release.Revision }}.db"
               cluster="{{ include "cray-etcd-base.fullname" . }}"

               echo "Patching snapshotter cronjob to remove labels to allow scheduling alongside etcd pods.."
               kubectl -n ${ns} patch cronjob ${cluster}-bitnami-etcd-snapshotter --type json -p='[{"op": "remove", "path": "/spec/jobTemplate/spec/template/metadata/labels/app.kubernetes.io~1instance"}]'

               kubectl -n ${ns} patch cronjob ${cluster}-bitnami-etcd-snapshotter --type json -p='[{"op": "remove", "path": "/spec/jobTemplate/spec/template/metadata/labels/app.kubernetes.io~1name"}]'

               cat > /tmp/upload-snapshot.py <<EOF
               #!/usr/bin/env python3

               import boto3
               import sys

               def main():
                   s3 = boto3.resource('s3', endpoint_url=sys.argv[1])
                   bucket = s3.Bucket('etcd-backup')
                   file_name="/snapshots/{{ include "cray-etcd-base.fullname" . }}-bitnami-etcd/%s" % (sys.argv[2])
                   key_name="{{ include "cray-etcd-base.fullname" . }}/%s" % (sys.argv[2])
                   bucket.upload_file(Filename=file_name, Key=key_name)

               if __name__ == '__main__':
                   main()
               EOF
               chmod 755 /tmp/upload-snapshot.py

               cat > /tmp/download-snapshot.py <<EOF
               #!/usr/bin/env python3
               
               import boto3
               import botocore
               import sys

               from botocore.config import Config

               S3_CONNECT_TIMEOUT=60
               S3_READ_TIMEOUT=1

               def main():

                   s3_config = Config(connect_timeout=S3_CONNECT_TIMEOUT, read_timeout=S3_READ_TIMEOUT)
                   s3 = boto3.resource('s3', endpoint_url=sys.argv[1], config=s3_config)
                   bucket = s3.Bucket('etcd-backup')
                   key_name="{{ include "cray-etcd-base.fullname" . }}/%s" % (sys.argv[3])
                   file_name="/%s/%s" % (sys.argv[2], sys.argv[3])
                   bucket.download_file(Filename=file_name, Key=key_name)
               
               if __name__ == '__main__':
                   main()
               EOF
               chmod 755 /tmp/download-snapshot.py

               echo "Grabbing keys and S3 endpoint from etcd-backup-s3-credentials..."
               access_key=$(kubectl get secret -n services etcd-backup-s3-credentials -ojsonpath='{.data.access_key}' | base64 -d)
               secret_key=$(kubectl get secret -n services etcd-backup-s3-credentials -ojsonpath='{.data.secret_key}' | base64 -d)
               http_s3_endpoint=$(kubectl get secret -n services etcd-backup-s3-credentials -ojsonpath='{.data.http_s3_endpoint}' | base64 -d)

               mkdir -p /root/.aws
               cat > /root/.aws/credentials <<EOF
               [default]
               aws_access_key_id = $access_key
               aws_secret_access_key = $secret_key
               EOF

               cat > /root/.aws/config <<EOF
               [default]
               region=foo
               EOF

               echo "Download snapshot ${snapshot_file} from ${http_s3_endpoint}..."
               /tmp/download-snapshot.py ${http_s3_endpoint} ${snapshot_dir} ${snapshot_file} 2>/dev/null

               if [ $? -ne 0 ]; then
                  echo "${snapshot_file} not found, proceeding without data migration..."
                  kubectl rollout status statefulset -n "${ns}" "${cluster}-bitnami-etcd"
                  echo "Taking initial snapshot of cluster..."
                  kubectl exec -i -n ${ns} "${cluster}-bitnami-etcd-0" -c etcd -- /bin/sh -c "ETCD_SNAPSHOTS_DIR=$snapshot_dir /opt/bitnami/scripts/etcd/snapshot.sh"
                  snapshot_file=$(find ${snapshot_dir}/ -maxdepth 1 -type f -name 'db-*' | sort | tail -n 1 | sed 's/.*\///')
                  echo "Uploading snapshot ${snapshot_file} to ${http_s3_endpoint}..."
                  /tmp/upload-snapshot.py ${http_s3_endpoint} ${snapshot_file}
                  echo "Editing etcd statefulset to existing (instead of new cluster)..."
                  kubectl set env statefulset -n "${ns}" "${cluster}-bitnami-etcd" ETCD_INITIAL_CLUSTER_STATE=existing ETCD_SNAPSHOTS_DIR=$snapshot_dir
                  exit 0
               fi

               echo "${snapshot_file} found, proceeding with data migration..."
               chown 1001:1001 ${snapshot_dir}/${snapshot_file}

               echo "Contents of $snapshot_dir:"
               ls -l $snapshot_dir

               echo "Scaling etcd statefulset down to zero..."
               kubectl scale statefulset -n "${ns}" "${cluster}-bitnami-etcd" --replicas=0
               kubectl rollout status statefulset -n "${ns}" "${cluster}-bitnami-etcd"

               kubectl set env statefulset -n "${ns}" "${cluster}-bitnami-etcd" ETCD_START_FROM_SNAPSHOT=yes ETCD_INIT_SNAPSHOT_FILENAME=$snapshot_file ETCD_INIT_SNAPSHOTS_DIR=$snapshot_dir

               pvc="data-${cluster}-bitnami-etcd"
               echo "Deleting existing PVC's..."
               kubectl delete pvc -n "${ns}" "${pvc}-0" "${pvc}-1" "${pvc}-2"

               echo "Scaling etcd statefulset back up to three members..."
               kubectl scale statefulset -n "${ns}" "${cluster}-bitnami-etcd" --replicas=3
               kubectl rollout status statefulset -n "${ns}" "${cluster}-bitnami-etcd" --timeout 120s

               if [ $? -ne 0 ]; then
                 echo "ERROR: Failed to restore from snapshot..."
                 exit 1
               fi

               echo "Deleting etcdbackup CRD from previous etcd deployment if present..."
               kubectl delete etcdbackup ${cluster}-etcd-cluster-periodic-backup -n ${ns} 2>/dev/null

               echo "Taking initial snapshot of cluster to $snapshot_dir..."
               kubectl exec -i -n ${ns} "${cluster}-bitnami-etcd-0" -c etcd -- /bin/sh -c "ETCD_SNAPSHOTS_DIR=$snapshot_dir /opt/bitnami/scripts/etcd/snapshot.sh"

               snapshot_file=$(find ${snapshot_dir}/ -maxdepth 1 -type f -name 'db-*' | sort | tail -n 1 | sed 's/.*\///')
               echo "Uploading snapshot ${snapshot_file} to ${http_s3_endpoint}..."
               /tmp/upload-snapshot.py ${http_s3_endpoint} ${snapshot_file}

               echo "Editing etcd statefulset to no longer start from snapshot..."

               kubectl set env statefulset -n "${ns}" "${cluster}-bitnami-etcd" ETCD_START_FROM_SNAPSHOT=no ETCD_INITIAL_CLUSTER_STATE=existing ETCD_SNAPSHOTS_DIR=$snapshot_dir

               echo "Done!"
